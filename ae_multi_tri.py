'''
  Simple Multi-Encoder Autoencoder for ECG subject-pattern disentanglement

  A subset of the data is processed within this script.

  modified from https://keras.io/examples/generative/vae/
'''

import keras
from keras.layers import Conv1D, Conv2DTranspose, Input, Flatten, Dense, Lambda, Reshape, UpSampling1D, AveragePooling1D
from keras.layers import BatchNormalization
from keras.models import Model
from keras.datasets import mnist
from keras.losses import binary_crossentropy, mean_squared_error, CategoricalCrossentropy
from keras import backend as K
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd


from utils import *
from config import get_config

#from tensorflow.python.framework.ops import disable_eager_execution
#disable_eager_execution()

config = get_config()

(X,y, group) = loaddata_nosplit_scaled_index(config.input_size, config.feature)
classes = ['A', 'E', 'j', 'L', 'N', 'P', 'R', 'V']
Xe = np.expand_dims(X, axis=2)

selected_beat_type = ['j']
#selected_subject = '207'

subject, subject_count = np.unique(group, return_counts = True)
marker = 0
beat_table = pd.DataFrame()
for c in subject_count:
    beat_table = pd.concat([beat_table, pd.DataFrame(y[marker:marker+c], columns=classes).sum()], axis=1)
    marker = marker+c
beat_table.columns = subject
beat_table = beat_table.T
print(beat_table)
beat_index_subject = np.array(beat_table.where(beat_table[selected_beat_type]>0).dropna().index)        #array of subjects with selected beat type

print(beat_index_subject)                               #subjects that have selected beat type
print(beat_table.where(beat_table[selected_beat_type]>0).sum())        #sum of beat count of subjects with selected type by beat type
print(beat_table.where(beat_table[selected_beat_type]>0).sum().sum())  #total sum of beat count of subjects with selected type
print("=========")
print(X.shape)                                          #original total beat count

X_subjects = []
y_subjects = []
X_beattype = []
y_beattype = []
X_subjects_beat = []
y_subjects_beat = []
X_trip_p = []
X_trip_s = []
X_trip_r = []

X_subjects_total = []
y_subjects_total = []
X_beattype_total = []
y_beattype_total = []
X_subjects_beat_total = []
y_subjects_beat_total = []
X_trip_p_total = []
X_trip_s_total = []
X_trip_r_total = []
for selected_beat_type in selected_beat_type:
    beat_index_subject = np.array(beat_table.where(beat_table[selected_beat_type]>0).dropna().index)        #array of subjects with selected beat type
    for selected_subject in beat_index_subject:
        for x in range(X.shape[0]):
            if group[x] == selected_subject:
                X_subjects.append(X[x])
                y_subjects.append(y[x])

            if group[x] in beat_index_subject and y[x][classes.index(selected_beat_type)] == 1:
                X_beattype.append(X[x])
                y_beattype.append(y[x])

            if group[x] == selected_subject and y[x][classes.index(selected_beat_type)] == 1:
                X_subjects_beat.append(X[x])
                y_subjects_beat.append(y[x])

            if group[x] in beat_index_subject and group[x] != selected_subject and y[x][classes.index(selected_beat_type)] == 1:
                X_trip_p.append(X[x])

            if group[x] == selected_subject and y[x][classes.index(selected_beat_type)] != 1:
                X_trip_s.append(X[x])

        selected_subject_count = np.array(X_subjects).shape[0] 
        print(selected_subject_count)                    #total beat count of selected subject
        beattype_count = np.array(X_beattype).shape[0]  
        print(beattype_count)                            #total beat count of subjects with selected beat type
        subject_beat_count = np.array(X_subjects_beat).shape[0]
        print(subject_beat_count)                        #beat count of selected subject and beat
        trip_p_count = np.array(X_trip_p).shape[0]
        trip_s_count = np.array(X_trip_s).shape[0]
        print(trip_p_count, trip_s_count)
        print("=========================")
        import random
        def random_oversample(data, label, count):
            r = random.randint(0,count-1)
            data.append(data[r])
            label.append(label[r])
            return data, label

        def random_oversample_1(data, count):
            r = random.randint(0,count-1)
            data.append(data[r])
            return data

        top_count = selected_subject_count if selected_subject_count > beattype_count else beattype_count
        if selected_subject_count > beattype_count:
            while selected_subject_count > beattype_count:
                X_beattype, y_beattype = random_oversample(X_beattype, y_beattype, beattype_count)
                beattype_count += 1
        else:
            while beattype_count > selected_subject_count:
                X_subjects, y_subjects = random_oversample(X_subjects, y_subjects, selected_subject_count)
                selected_subject_count += 1

        while top_count > subject_beat_count:
            X_subjects_beat, y_subjects_beat = random_oversample(X_subjects_beat, y_subjects_beat, subject_beat_count)
            subject_beat_count += 1

        if trip_p_count > top_count:
            X_trip_p_temp = []
            X_trip_p_temp_count = 0
            while top_count > X_trip_p_temp_count:
                X_trip_p_temp = random_oversample_1(X_trip_p, trip_p_count)
                X_trip_p_temp_count += 1
            X_trip_p = X_trip_p_temp

        while top_count > trip_p_count:
            X_trip_p = random_oversample_1(X_trip_p, trip_p_count)
            trip_p_count += 1


        if trip_s_count > top_count:
            X_trip_s_temp = []
            X_trip_s_temp_count = 0
            while top_count > X_trip_s_temp_count:
                X_trip_s_temp = random_oversample_1(X_trip_p, trip_p_count)
                X_trip_s_temp_count += 1
            X_trip_s_temp = X_trip_s_temp

        while top_count > trip_s_count:
            X_trip_s = random_oversample_1(X_trip_s, trip_s_count)
            trip_s_count += 1

        for x in range(X.shape[0]):
            if group[x] != selected_subject and y[x][classes.index(selected_beat_type)] != 1:
                r = random.randint(0,1)
                if (r == 1): X_trip_r.append(X[x]) 
            if np.array(X_trip_r).shape[0] >= top_count:
                break
        trip_ref_count = np.array(X_trip_r).shape[0]

        print(top_count)
        def collect_values(collector, data):
            return collector.append(data)

        collect_values(X_subjects_total, X_subjects)
        collect_values(y_subjects_total, y_subjects)
        collect_values(X_beattype_total, X_beattype)
        collect_values(y_beattype_total, y_beattype)
        collect_values(X_subjects_beat_total, X_subjects_beat)
        collect_values(y_subjects_beat_total, y_subjects_beat)
        collect_values(X_trip_p_total, X_trip_p)
        collect_values(X_trip_s_total, X_trip_s)
        collect_values(X_trip_r_total, X_trip_r)


X_test = np.array([np.array(X_beattype_total), np.array(X_subjects_total), np.array(X_subjects_beat_total), np.array(X_trip_p_total), np.array(X_trip_s_total), np.array(X_trip_r_total)])
y_test = np.array([np.array(y_beattype_total), np.array(y_subjects_total), np.array(y_subjects_beat_total)])
print(X_test.shape, y_test.shape)
X_test = X_test.reshape(X_test.shape[0], X_test.shape[1]*X_test.shape[2], X_test.shape[3])
y_test = y_test.reshape(y_test.shape[0], y_test.shape[1]*y_test.shape[2], y_test.shape[3])
print(X_test.shape, y_test.shape)
print("=========================")


X_test_temp = []
for x in X_test:
  X_test_temp.append(np.expand_dims(x, axis=2))
X_test = X_test_temp

y_test_temp = []
for y in y_test:
  y_test_temp.append(np.array(pd.DataFrame(y).idxmax(axis=1)))
y_test = y_test_temp

target_train = y_test
# Data & model configuration
batch_size = config.batch
no_epochs = config.ae_epochs
validation_split = 0.25
verbosity = 1
latent_dim = 2
num_channels = 1

# Reshape data

input_train = X_test
input_shape = (config.input_size, 1)


# # =================
# # Encoder
# # =================

# Definition
i       = Input(shape=input_shape, name='encoder_input')
cx      = Conv1D(filters=8, kernel_size=16, strides=2, padding='same', activation='relu')(i)
cx      = BatchNormalization()(cx)
cx      = Conv1D(filters=16, kernel_size=16, strides=2, padding='same', activation='relu')(cx)
cx      = BatchNormalization()(cx)
cx      = Conv1D(filters=16, kernel_size=16, strides=2, padding='same', activation='relu')(cx)
cx      = BatchNormalization()(cx)
cx      = Conv1D(filters=1, kernel_size=16, strides=2, padding='same', activation='relu')(cx)
eo      = BatchNormalization()(cx)

conv_shape = K.int_shape(cx)
print(conv_shape)

# Instantiate encoder
encoder = Model(i, eo, name='encoder')
#encoder = Model(i, [mu, sigma, z], name='encoder')
encoder.summary()

# # =================
# # Encoder_2
# # =================

# Definition
i_2       = Input(shape=input_shape, name='encoder2_input')
cx      = Conv1D(filters=8, kernel_size=16, strides=2, padding='same', activation='relu')(i_2)
cx      = BatchNormalization()(cx)
cx      = Conv1D(filters=16, kernel_size=16, strides=2, padding='same', activation='relu')(cx)
cx      = BatchNormalization()(cx)
cx      = Conv1D(filters=16, kernel_size=16, strides=2, padding='same', activation='relu')(cx)
cx      = BatchNormalization()(cx)
cx      = Conv1D(filters=1, kernel_size=16, strides=2, padding='same', activation='relu')(cx)
eo_2      = BatchNormalization()(cx)


# Get Conv2D shape for Conv2DTranspose operation in decoder
conv_shape_2 = K.int_shape(cx)

# Instantiate encoder
encoder_2 = Model(i_2, eo_2, name='encoder_2')
encoder_2.summary()

# =================
# Decoder
# =================

# Definition
d_i   = Input(shape=(conv_shape[1], conv_shape[2]*2), name='decoder_input')
cx    = UpSampling1D(size=2)(d_i)
cx    = Conv1D(filters=2, kernel_size=16, strides=2, padding='same', activation='relu')(cx)
cx    = BatchNormalization()(cx)
cx    = UpSampling1D(size=2)(cx)
cx    = Conv1D(filters=2, kernel_size=16, strides=2, padding='same', activation='relu')(cx)
cx    = BatchNormalization()(cx)
cx    = UpSampling1D(size=2)(cx)
cx    = Conv1D(filters=1, kernel_size=16, strides=2, padding='same',  activation='relu', name = 'conv12d3')(cx)
cx    = BatchNormalization()(cx)
cx    = UpSampling1D(size=4)(cx)
cx    = Conv1D(filters=1, kernel_size=16, strides=2, padding='same',  activation='relu', name = 'conv12d4')(cx)
cx    = BatchNormalization()(cx)
cx    = UpSampling1D(size=4)(cx)
cx    = Conv1D(filters=num_channels, kernel_size=16, activation='relu', padding='same', name='decoder_output')(cx)
o     = UpSampling1D(size=2)(cx)

# Instantiate decoder
decoder = Model(d_i, o, name='decoder')
decoder.summary()

# =================
# VAE as a whole
# =================
import tensorflow as tf

class AE(keras.Model):
    def __init__(self, encoder, encoder_2, decoder, **kwargs):
        super(AE, self).__init__(**kwargs)
        self.encoder = encoder
        self.encoder_2 = encoder_2
        self.decoder = decoder
    
    def call(self, inputs):
        return self.decoder(tf.concat([self.encoder(inputs), self.encoder_2(inputs)], 2))

    def train_step(self, data):
        data = data[0]
        data_1 = data[1]
        true = data[2]
        data_p = data[3]
        data_s = data[4]
        data_r = data[5]

        with tf.GradientTape() as tape:
            encoder_output = encoder(data)
            encoder_2_output = encoder_2(data_1)
            reconstruction = decoder(tf.concat([encoder_output, encoder_2_output], 2))
            
            reconstruction_loss = tf.reduce_mean(
                mean_squared_error(true, reconstruction)
            )
            cross_recon_loss = tf.reduce_mean(
                true -  reconstruction
            )
            alpha = 0.2
            trip_p_loss = tf.reduce_mean(abs(encoder(data)-encoder(data_p))- abs(encoder(data)-encoder(data_r)) + alpha)    #encoder()
            trip_s_loss = tf.reduce_mean(abs(encoder_2(data_1)-encoder_2(data_s))- abs(encoder_2(data_1)-encoder_2(data_r)) + alpha)
            total_loss = reconstruction_loss + cross_recon_loss + trip_s_loss + trip_p_loss
        grads = tape.gradient(total_loss, self.trainable_weights)
        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))
        return {
            "loss": total_loss,
            "reconstruction_loss": reconstruction_loss,
            "cross_recon_loss": cross_recon_loss,
            "trip_s_loss": trip_s_loss,
            "trip_p_loss": trip_p_loss
        }


# Instantiate AE
vae         = AE(encoder, encoder_2, decoder, name='multi-ae')

# Compile VAE
vae.compile(optimizer=keras.optimizers.Adam())

# Train autoencoder
vae.fit([input_train[0], input_train[1], input_train[2], input_train[3], input_train[4], input_train[5]], epochs = no_epochs, batch_size = batch_size, validation_split = validation_split)
vae.summary()



# =================
# Results visualization
# Credits for original visualization code: https://keras.io/examples/variational_autoencoder_deconv/
# (François Chollet).
# =================
def viz_latent_space(encoder, data):
  input_data, target_data = data
  print("tsne plot")
  #print(target_data.shape)
  #print(target_data.shape[0])
  #print(target_data)
  #print(encoder.predict(input_data).shape)#.reshape((32,16))
  #print(encoder.predict(input_data).reshape(input_data.shape[0], 16).shape)#.reshape((32,16))

  from sklearn.manifold import TSNE
  X_tsne = TSNE(n_components=2, random_state=1).fit_transform(encoder.predict(input_data).reshape(input_data.shape[0], 16))
  #print(X_tsne.shape)
  #print(X_tsne)


  plt.figure(figsize=(8, 10))
  scatter = plt.scatter(X_tsne[:,0], X_tsne[:,1], c=target_data, label = classes)
  plt.legend(handles=scatter.legend_elements()[0], labels=classes)
  plt.title("tsne")
  plt.show()

def viz_latent_space_pca(encoder, data):
  input_data, target_data = data
  print('pca plot')

  from sklearn.decomposition import PCA
  principalComponents = PCA(n_components=2, random_state = 1).fit_transform(encoder.predict(input_data).reshape(input_data.shape[0], 16))

  
  plt.figure(figsize=(8, 10))

  scatter = plt.scatter((principalComponents[:,0]), (principalComponents[:,1]), c=target_data, label=classes)
  plt.legend(handles=scatter.legend_elements()[0], labels=classes)
  plt.title("pca")
  plt.show()

def plot_some_signals(vae, data):
    input_data, target_data = data
    x_vae_pred = vae.predict(input_data, input_data)

    from matplotlib import pyplot as plt
    xaxis = np.arange(0,config.input_size)
    for count in range(5):
        plt.plot(xaxis, x_vae_pred[count])
    plt.title("ae reconstructed beats")
    plt.xlabel("beat length")
    plt.ylabel("signal")
    plt.show()

# Plot results
input_test = np.concatenate((X_test[0], X_test[1]))
target_test = np.concatenate((y_test[0], y_test[1]))
data = (input_test, target_test)
viz_latent_space_pca(encoder, data)
viz_latent_space_pca(encoder_2, data)

target_train = np.array(pd.get_dummies(pd.DataFrame(target_train)))
target_test = np.array(pd.get_dummies(pd.DataFrame(target_test)))

(m, n) = target_train.shape
target_train = target_train.reshape((m, 1, n ))
(mvl, nvl) = target_test.shape
target_test = target_test.reshape((mvl, 1, nvl))

# Definition
i_c     = Input(shape=(conv_shape[1],conv_shape[2]), name='classifier_input')
cx      = Reshape((conv_shape[1],))(i_c)
co      = Dense(len(classes), activation='softmax')(cx)

# Instantiate encoder
classifier = Model(i_c, co, name='classifier')
classifier.summary()
classifier.compile(optimizer=keras.optimizers.Adam(), loss = tf.keras.losses.CategoricalCrossentropy())
classifier.fit(encoder(input_train[0]), target_train[0], epochs = no_epochs, validation_split = validation_split)
print_results_ae_multi(config, classifier, encoder(input_test), target_test.reshape(target_test.shape[0],target_test.shape[1]*target_test.shape[2]), classes)
